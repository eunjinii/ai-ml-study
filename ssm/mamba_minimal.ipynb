{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eunjin/anaconda3/envs/study/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mamba import Mamba, ModelArgs\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eunjin/anaconda3/envs/study/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Downloading tokenizer_config.json: 100%|██████████| 156/156 [00:00<00:00, 35.7kB/s]\n",
      "Downloading vocab.json: 100%|██████████| 1.08M/1.08M [00:00<00:00, 1.44MB/s]\n",
      "Downloading merges.txt: 100%|██████████| 457k/457k [00:00<00:00, 1.17MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 2.11M/2.11M [00:01<00:00, 2.08MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 108kB/s]\n",
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = 'state-spaces/mamba-370m'\n",
    "model = Mamba.from_pretrained(pretrained_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, \n",
    "\t\t\t tokenizer, \n",
    "\t\t\t prompt:str,\n",
    "\t\t\t n_tokens_to_gen: int=50,\n",
    "\t\t\t sample: bool=True,\n",
    "\t\t\t top_k: int=40):\n",
    "\tmodel.eval()\n",
    "\tinput_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\t\n",
    "\tfor token_n in range(n_tokens_to_gen):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tindices_to_input = input_ids\n",
    "\t\t\tnext_token_logits = model(indices_to_input)[:, -1]\n",
    "\n",
    "\t\tprobs = F.softmax(next_token_logits, dim=-1)\n",
    "\t\t(batch, vocab_size) = probs.shape\n",
    "\t\n",
    "\t\tif top_k is not None:\n",
    "\t\t\t(values, indices) = torch.topk(probs, k=top_k)\n",
    "\t\t\tprobs[probs < values[:, -1, None]] = 0\n",
    "\t\t\tprobs = probs / probs.sum(axis=1, keepdims=True)\n",
    "   \n",
    "\t\tif sample:\n",
    "\t\t\tnext_indices = torch.multinomial(probs, num_samples=1)\n",
    "\t\telse:\n",
    "\t\t\tnext_indices = torch.argmax(probs, dim=-1)[:, None]\n",
    "\n",
    "\t\tinput_ids = torch.cat([input_ids, next_indices], dim=1)\n",
    "\t\n",
    "\toutput_completions = [tokenizer.decode(output.tolist()) for output in input_ids][0]\n",
    "\t\n",
    "\treturn output_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 00:22:15.955730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-01 00:22:17.083428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mamba is the third of a planned seven-film series.\n",
      "\n",
      "According to a press note in the magazine, the first film in the universe, which includes the first 10 episodes, \"was released in the U.S. July 1, 1988, with foreign\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'Mamba is the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U of T grad students are working with the community.\n",
      "<dpm> I think it's a good idea in the long run. I think it's the right place to host it. There are a lot of people willing to host this\n",
      "<tvansteenburgh\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, 'U of T grad students are'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
